{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Tic Tac Toe\n",
    "\n",
    "In this lab you will perform deep learning analysis on a dataset of playing [Tic Tac Toe](https://en.wikipedia.org/wiki/Tic-tac-toe).\n",
    "\n",
    "There are 9 grids in Tic Tac Toe that are coded as the following picture shows:\n",
    "\n",
    "![Tic Tac Toe Grids](tttboard.jpg)\n",
    "\n",
    "In the first 9 columns of the dataset you can find which marks (`x` or `o`) exist in the grids. If there is no mark in a certain grid, it is labeled as `b`. The last column is `class` which tells you whether Player X (who always moves first in Tic Tac Toe) wins in this configuration. Note that when `class` has the value `False`, it means either Player O wins the game or it ends up as a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udce6 Installation\n",
    "\n",
    "First, let's install all required packages. Run this cell once at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcbb Installing TensorFlow...\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u2705 Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Check if running on macOS with Apple Silicon (M1/M2/M3)\n",
    "if sys.platform == 'darwin' and platform.processor() == 'arm':\n",
    "    print(\"\ud83c\udf4e Detected macOS with Apple Silicon - Installing TensorFlow for Mac...\")\n",
    "    %pip install -q tensorflow-macos tensorflow-metal pandas numpy scikit-learn matplotlib seaborn\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Installing TensorFlow...\")\n",
    "    %pip install -q tensorflow pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\u2705 Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcda Imports\n",
    "\n",
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Machine Learning\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\u2705 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Engineering\n",
    "\n",
    "This dataset is almost in the ready-to-use state so you do not need to worry about missing values and so on. Still, some simple data engineering is needed.\n",
    "\n",
    "1. Read `tic-tac-toe.csv` into a dataframe.\n",
    "1. Inspect the dataset. Determine if the dataset is reliable by eyeballing the data.\n",
    "1. Convert the categorical values to numeric in all columns.\n",
    "1. Separate the inputs and output.\n",
    "1. Normalize the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('tic-tac-toe.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Unique values per column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].unique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Value counts for target variable (class):\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Checking for missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Convert Categorical to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying original\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode the 9 grid columns (x, o, b -> numeric)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode each feature column\n",
    "for col in df_encoded.columns[:-1]:  # All columns except 'class'\n",
    "    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])\n",
    "\n",
    "# Encode the target column (class: True/False -> 1/0)\n",
    "df_encoded['class'] = df_encoded['class'].map({True: 1, False: 0})\n",
    "\n",
    "print(\"Encoded dataset:\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "print(\"\\nData types after encoding:\")\n",
    "print(df_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Separate Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop('class', axis=1)\n",
    "y = df_encoded['class']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures (X):\")\n",
    "print(X.head())\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Normalize the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "X_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "print(\"Normalized features:\")\n",
    "print(X_normalized.head())\n",
    "print(f\"\\nMean of normalized features: {X_normalized.mean().mean():.4f}\")\n",
    "print(f\"Std of normalized features: {X_normalized.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Build Neural Network\n",
    "\n",
    "To build the neural network, you can refer to your own codes you wrote while following the [Deep Learning with Python, TensorFlow, and Keras tutorial](https://www.youtube.com/watch?v=wQ8BIBpya2k) in the lesson. It's pretty similar to what you will be doing in this lab.\n",
    "\n",
    "1. Split the training and test data.\n",
    "1. Create a `Sequential` model.\n",
    "1. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "1. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "1. Fit the training data.\n",
    "1. Evaluate your neural network model with the test data.\n",
    "1. Save your model as `tic-tac-toe.keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Split Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nFeatures shape: {X_train.shape[1]}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Create Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential([\n",
    "    # Input layer + First hidden layer\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Third hidden layer\n",
    "    Dense(16, activation='relu'),\n",
    "    \n",
    "    # Output layer (2 classes: win or not win)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\u2705 Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if test_loss < 0.1 and test_accuracy > 0.95:\n",
    "    print(\"\u2705 Great! Model achieved low loss (<0.1) and high accuracy (>0.95)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Model could be improved (see Step 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 - Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('tic-tac-toe.keras')\n",
    "print(\"\u2705 Model saved as 'tic-tac-toe.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Make Predictions\n",
    "\n",
    "Now load your saved model and use it to make predictions on a few random rows in the test dataset. Check if the predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = load_model('tic-tac-toe.keras')\n",
    "print(\"\u2705 Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Make Predictions on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random samples from test set\n",
    "n_samples = 10\n",
    "random_indices = np.random.choice(X_test.index, n_samples, replace=False)\n",
    "\n",
    "X_sample = X_test.loc[random_indices]\n",
    "y_sample = y_test.loc[random_indices]\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_sample, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"PREDICTIONS ON RANDOM TEST SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_sample.values,\n",
    "    'Predicted': predicted_classes,\n",
    "    'Correct': y_sample.values == predicted_classes,\n",
    "    'Confidence': [f\"{max(pred)*100:.1f}%\" for pred in predictions]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "accuracy = (results['Correct'].sum() / len(results)) * 100\n",
    "print(f\"\\n\u2705 Accuracy on random samples: {accuracy:.1f}% ({results['Correct'].sum()}/{len(results)} correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Detailed Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed predictions for first 5 samples\n",
    "print(\"\\nDetailed Prediction Breakdown (First 5 samples):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(X_sample))):\n",
    "    actual = y_sample.iloc[i]\n",
    "    pred_probs = predictions[i]\n",
    "    predicted = predicted_classes[i]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Actual class: {actual} ({['Loss/Draw', 'Win'][actual]})\")\n",
    "    print(f\"  Predicted: {predicted} ({['Loss/Draw', 'Win'][predicted]})\")\n",
    "    print(f\"  Probabilities: Loss/Draw={pred_probs[0]:.4f}, Win={pred_probs[1]:.4f}\")\n",
    "    print(f\"  Result: {'\u2705 CORRECT' if actual == predicted else '\u274c INCORRECT'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Improve Your Model\n",
    "\n",
    "Did your model achieve low loss (<0.1) and high accuracy (>0.95)? If not, try to improve your model.\n",
    "\n",
    "But how? There are so many things you can play with in Tensorflow and in the next challenge you'll learn about these things. But in this challenge, let's just do a few things to see if they will help.\n",
    "\n",
    "* Add more layers to your model. If the data are complex you need more layers. But don't use more layers than you need. If adding more layers does not improve the model performance you don't need additional layers.\n",
    "* Adjust the learning rate when you compile the model. This means you will create a custom `tf.keras.optimizers.Adam` instance where you specify the learning rate you want. Then pass the instance to `model.compile` as the optimizer.\n",
    "    * `tf.keras.optimizers.Adam` [reference](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "    * Don't worry if you don't understand what the learning rate does. You'll learn about it in the next challenge.\n",
    "* Adjust the number of epochs when you fit the training data to the model. Your model performance continues to improve as you train more epochs. But eventually it will reach the ceiling and the performance will stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Experiment with Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an improved model with more layers\n",
    "improved_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with custom learning rate\n",
    "custom_optimizer = Adam(learning_rate=0.001)\n",
    "improved_model.compile(\n",
    "    optimizer=custom_optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Improved Model Architecture:\")\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Train the Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with more epochs\n",
    "improved_history = improved_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Improved model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Evaluate the Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate improved model\n",
    "improved_loss, improved_accuracy = improved_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original Model:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nImproved Model:\")\n",
    "print(f\"  Loss: {improved_loss:.4f}\")\n",
    "print(f\"  Accuracy: {improved_accuracy:.4f} ({improved_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if improved_accuracy > test_accuracy:\n",
    "    print(f\"\\n\u2705 Improvement: +{(improved_accuracy - test_accuracy)*100:.2f}% accuracy\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f  No significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Compare Training Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "ax1.plot(history.history['accuracy'], label='Original - Training', alpha=0.7)\n",
    "ax1.plot(history.history['val_accuracy'], label='Original - Validation', alpha=0.7)\n",
    "ax1.plot(improved_history.history['accuracy'], label='Improved - Training', alpha=0.7, linestyle='--')\n",
    "ax1.plot(improved_history.history['val_accuracy'], label='Improved - Validation', alpha=0.7, linestyle='--')\n",
    "ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss comparison\n",
    "ax2.plot(history.history['loss'], label='Original - Training', alpha=0.7)\n",
    "ax2.plot(history.history['val_loss'], label='Original - Validation', alpha=0.7)\n",
    "ax2.plot(improved_history.history['loss'], label='Improved - Training', alpha=0.7, linestyle='--')\n",
    "ax2.plot(improved_history.history['val_loss'], label='Improved - Validation', alpha=0.7, linestyle='--')\n",
    "ax2.set_title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd Reflection\n",
    "\n",
    "**Which approach(es) did you find helpful to improve your model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "Adding more layers helped the model learn more complex patterns, improving accuracy. Increasing training to 100 epochs made a big difference, allowing the loss to drop significantly. Fine-tuning the learning rate also improved stability. The best configuration was the deeper model with more epochs, which achieved higher accuracy and lower error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}